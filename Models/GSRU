import torch
import torch.nn as nn
from torch.nn import functional as F

class GCN(nn.Module):
    """ Graph convolution unit (single layer)
    """

    def __init__(self, num_state, num_node, bias=False):
        super(GCN, self).__init__()
        self.conv1 = nn.Conv1d(num_state, num_node, kernel_size=1)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv1d(num_node, num_state, kernel_size=1, bias=bias)

    def forward(self, x):
        h = self.conv1(x)
        h_out = self.conv2(self.relu(h)) + x
        return h_out


class GloRe_Unit(nn.Module):

    def __init__(self, num_in, num_mid,
                 ConvNd=nn.Conv3d,
                 BatchNormNd=nn.BatchNorm3d,
                 normalize=False):
        super(GloRe_Unit, self).__init__()

        self.normalize = normalize
        self.num_s = int(2 * num_mid)
        self.num_n = int(1 * num_mid)

        self.conv_state = ConvNd(num_in, self.num_s, kernel_size=1)
        self.conv_state_sem1 = ConvNd(num_in, self.num_s, kernel_size=1)
        self.conv_state_sem2 = ConvNd(num_in, self.num_s, kernel_size=1)
        self.conv_proj = ConvNd(num_in, self.num_n, kernel_size=1)

        self.gcn = GCN(num_state=self.num_s, num_node=self.num_n)

        self.conv_extend = ConvNd(self.num_s, num_in, kernel_size=1, bias=False)

        self.blocker = BatchNormNd(num_in, eps=1e-04)


    def forward(self, x, sem):

        n = x.size(0)

        x_state_reshaped_o = self.conv_state(x).view(n, self.num_s, -1)

        sem_state_1 = self.conv_state_sem1(sem).view(n, self.num_s, -1)

        sem_state_2 = self.conv_state_sem2(sem).view(n, -1, self.num_s)

        sem_state = F.softmax(torch.matmul(sem_state_2, sem_state_1), dim=-1)

        x_gcn = torch.matmul(x_state_reshaped_o, sem_state)

        if self.normalize:
            x_gcn = x_gcn * (1. / x_gcn.size(2))

        x_n_rel = self.gcn(x_gcn)

        out = x + self.blocker(self.conv_extend(x_n_rel.view(n, self.num_s, x.size(2), x.size(3))))

        return out




class GCN_Unit_2D(GloRe_Unit):
    def __init__(self, num_in, num_mid, normalize=False):
        super(GCN_Unit_2D, self).__init__(num_in, num_mid,
                                            ConvNd=nn.Conv2d,
                                            BatchNormNd=nn.BatchNorm2d,
                                            normalize=normalize)


